<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav ul li a { background-image: none; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Colab으로 파인튜닝 Fine-Tuning 하기 | Rachel Docs</title> <meta name="generator" content="Jekyll v4.3.4" /> <meta property="og:title" content="Colab으로 파인튜닝 Fine-Tuning 하기" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="이 글은 연세대학교 AIC3110 강의를 참고하였으며, 허가 하에 작성되었습니다." /> <meta property="og:description" content="이 글은 연세대학교 AIC3110 강의를 참고하였으며, 허가 하에 작성되었습니다." /> <link rel="canonical" href="http://localhost:4000/colab/2024/12/29/Colab%EC%9C%BC%EB%A1%9C-%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D-Fine-Tuning-%ED%95%98%EA%B8%B0.html" /> <meta property="og:url" content="http://localhost:4000/colab/2024/12/29/Colab%EC%9C%BC%EB%A1%9C-%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D-Fine-Tuning-%ED%95%98%EA%B8%B0.html" /> <meta property="og:site_name" content="Rachel Docs" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2024-12-29T00:00:00+09:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Colab으로 파인튜닝 Fine-Tuning 하기" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-12-29T00:00:00+09:00","datePublished":"2024-12-29T00:00:00+09:00","description":"이 글은 연세대학교 AIC3110 강의를 참고하였으며, 허가 하에 작성되었습니다.","headline":"Colab으로 파인튜닝 Fine-Tuning 하기","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/colab/2024/12/29/Colab%EC%9C%BC%EB%A1%9C-%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D-Fine-Tuning-%ED%95%98%EA%B8%B0.html"},"url":"http://localhost:4000/colab/2024/12/29/Colab%EC%9C%BC%EB%A1%9C-%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D-Fine-Tuning-%ED%95%98%EA%B8%B0.html"}</script> <!-- End Jekyll SEO tag --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Rachel Docs </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Rachel Docs" aria-label="Search Rachel Docs" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/just-the-docs/just-the-docs-template" class="site-button" > Template Repository </a> </li> </ul> </nav> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <p>이 글은 연세대학교 AIC3110 강의를 참고하였으며, 허가 하에 작성되었습니다.</p> <p>프롬프팅에 이어 파인튜닝을 해보려 한다 !</p> <h2 id="1-파인튜닝이란"> <a href="#1-파인튜닝이란" class="anchor-heading" aria-labelledby="1-파인튜닝이란"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. 파인튜닝이란? </h2> <p>파인튜닝이란 처음부터 데이터를 쌓아서 학습시키는 것이 아니라, 원래 있던 모델을 목적에 맞게 추가적인 데이터를 통해 학습시켜 특정 태스크를 잘할 수 있는 모델로 성장시키는 과정이다.</p> <p>당연히, high-quality 데이터들이 필요할 것이다. high-quality 데이터란 ,, 거의 완전히 real data 즉 인간 데이터이다. 그치만 이런 데이터들은 보안 문제도 있고, 무제한 생성하기도 어렵고, 개인 사생활 문제도 있다.</p> <p>그래서 요즘은 <strong>synthetic data</strong> 를 통해 대체하는 추세이다. LLM을 통해 synthetic을 만들어 human-generate data를 대체한다.</p> <p>Prompt Chaining을 통해, 큰 모델을 통해 데이터셋을 만들고 그것을 작은 모델에 학습시켜보는 과정을 진행해보려 한다.</p> <h2 id="2-파인튜닝을-해보자"> <a href="#2-파인튜닝을-해보자" class="anchor-heading" aria-labelledby="2-파인튜닝을-해보자"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. 파인튜닝을 해보자 </h2> <p>목적은, 심리상담 챗봇을 만드는 것이다. 그렇다면 우선 심리 상담 데이터가 필요할 것이다.</p> <h3 id="1-심리-상담이-무엇인가-"> <a href="#1-심리-상담이-무엇인가-" class="anchor-heading" aria-labelledby="1-심리-상담이-무엇인가-"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> (1) 심리 상담이 무엇인가 ? </h3> <p>우리가 만들고자 하는 심리 상담 데이터는 현실 상담 시나리오랑 비슷하게 만드는 것이 목표이다.</p> <p>그렇다면 현실 상담이란 무엇인가?</p> <ul> <li>Multi-turn 으로 진행한다</li> <li>CBT(인지행동치료)를 적용한다</li> </ul> <p>사실 모든 치료에 대해 CBT가 근거기반치료인 것은 아니다.. (아직 연구되고 있는 분야가 많음) 그치만 CBT는 거의 모든 임상군에게 적용되었을 때 유의미한 효과를 나타내기 때문에 CBT를 특징으로 잡고 가도 크게 임상적 문제는 없을 것이라고 생각이 된다</p> <p><strong>PatternReframe</strong>이라는 데이터셋을 사용해볼 것이다 (Maddela et al)<strong>.</strong> Persona, Negative Thought, Patters, Reframed Thought 4가지 요소로 구분되어 있다. 이 데이터셋을 이용해서 client를 시뮬레이션해보려 한다. 그 중 하나를 가져왔다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">user_data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">persona</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">i love computers. i</span><span class="sh">'</span><span class="s">m very good at math and science . i started working at google last week on self driving car research . i i love logical and rational thinking .</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">thought</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">I was rejected by a woman. I am sure it is because she like tough guys and not nerds.</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">reframes</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
            <span class="sh">"</span><span class="s">I was rejected by a woman but who cares as there</span><span class="sh">'</span><span class="s">s someone for everyone and I can meet others!</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">I was rejected by a woman. I think I will find someone better soon.</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">I was rejected by a woman but thats ok ill find a better match soon!</span><span class="sh">"</span>
        <span class="p">],</span>
        <span class="sh">"</span><span class="s">patterns</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
            <span class="sh">"</span><span class="s">mental filtering</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">jumping to conclusions: mind reading</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">overgeneralization</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">black-and-white or polarized thinking / all or nothing thinking</span><span class="sh">"</span>
        <span class="p">]</span>
    <span class="p">}</span>
</code></pre></div></div> <p>persona, negative thought 등의 요소로 이루어져 있다. negative thought → reframes가 CBT의 목표라고 생각하면 된다. pattern의 경우 thought이 포함하고 있는 인지 오류들이다.</p> <h3 id="2-심리-상담-데이터를-구조화해보자"> <a href="#2-심리-상담-데이터를-구조화해보자" class="anchor-heading" aria-labelledby="2-심리-상담-데이터를-구조화해보자"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> (2) 심리 상담 데이터를 구조화해보자 </h3> <p>심리 상담 데이터를 구조화하기 위해 다음과 같이 세분화하였다.</p> <ul> <li>Client-side Simulation : LLM에게 PatternReframe 기반으로 정보를 제공한 뒤, 이를 가지고 psychology Intake Form을 적어보도록 함</li> <li>Counselor-side Simulation : Client 상황에 맞는 CBT 기법을 고르고 plan하도록 함. Plan-and-Solve Prompting에서 근거를 찾을 수 있을 것</li> <li>Dialogue Generation : Script Mode를 사용할 것 <ul> <li>Script Mode : 한 모델이 양쪽의 대화를 모두 생성함 (자연스러운 대화가 나옴) .</li> <li>Two-agent Mode : 두 모델이 각각 역할을 맡아 대화를 생성함</li> </ul> </li> </ul> <blockquote> <p>Client side simulation (Form 형성) → CBT 기법 고르기 → 상담 plan → 대화 생성</p> </blockquote> <p>일반적으로 LLM에게 한 번에 시키면 잘 못하는 것들을, 태스크를 여러 개로 쪼개고 프롬프팅 기법을 통해 더 좋은 품질의 데이터셋을 만들기 위해 노력한 과정이라고 이해하면 좋을 것 같다.</p> <ol> <li>client-side Simulation</li> </ol> <p>우선 클라이언트를 모델링해보자. 이 클라이언트가 왜 상담하러 온 것인지 자세히 적는 것이다. User의 정보를 통해서 클라이언트에 대한 정보나, 왜 상담에 왔으며, 어떤 문제를 가졌는지 등 정보들을 포함해서 적도록 한다. 이런 식으로 example을 준 후, 이 형식에 따라서 적어주도록 했다(one-shot). 모델은 <code class="language-plaintext highlighter-rouge">llama3-70b-8192</code>를 이용했다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1. Client modeling
</span><span class="n">persona</span> <span class="o">=</span> <span class="n">user_data</span><span class="p">[</span><span class="sh">'</span><span class="s">persona</span><span class="sh">'</span><span class="p">]</span>
<span class="n">thought</span> <span class="o">=</span> <span class="n">user_data</span><span class="p">[</span><span class="sh">'</span><span class="s">thought</span><span class="sh">'</span><span class="p">]</span>
<span class="n">patterns</span> <span class="o">=</span> <span class="n">user_data</span><span class="p">[</span><span class="sh">'</span><span class="s">patterns</span><span class="sh">'</span><span class="p">]</span>

<span class="n">intake_form_generation_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'''</span><span class="s">
Thought depicts a situation where cognitive distortions exhibited by the client have caused problems in daily life, and patterns refer to the types of cognitive distortions the client possesses.

Please generate a client intake form ...

1. Basic Information
- occupation, ...

2. Presenting Problem
- What issue/symptoms ... 

3. Reason for Seeking Counseling
- What was the ... 

4. Past History (including medical history)
- Have you experienced ... 

5. Academic/occupational functioning level (attendance, grades/job performance, etc.)
- Interpersonal ... 

6. Is there anyone you can talk to or get help from when you encounter difficulties or problems?

## Example 1
~~~

## Example 2
[Persona]
</span><span class="si">{</span><span class="n">persona</span><span class="si">}</span><span class="s">

[Thought]
</span><span class="si">{</span><span class="n">thought</span><span class="si">}</span><span class="s">

[Patterns]
</span><span class="si">{</span><span class="n">patterns</span><span class="si">}</span><span class="s">

[Client Intake Form]</span><span class="sh">'''</span>

<span class="n">intake_form</span> <span class="o">=</span> <span class="nf">generate_response</span><span class="p">(</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">intake_form_generation_prompt</span><span class="p">,</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llama3-70b-8192</span><span class="sh">"</span>
<span class="p">)</span>
<span class="n">intake_form</span>
</code></pre></div></div> <p>내용이 너무 길어서 중략시켰다. 사실 지금 내용 자체가 중요하지는 않다.</p> <p>이제 이 인지오류를 겪는 사람에게 어떤 CBT가 적합할지 찾는 과정을 진행해보자. 이를 <code class="language-plaintext highlighter-rouge">cbt_tech_generation_prompt</code> 로 지정한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 2. CBT Technique Generation
</span><span class="n">cbt_tech_generation_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'''</span><span class="s">
You are a counselor specializing in CBT techniques. Choose top 1 appropriate CBT technique from the given CBT techniques to use with the client based on their intake form. Output only the name of the CBT techniques.

[Types of CBT Techniques]
Efficiency Evaluation, Pie Chart Technique, Alternative Perspective, Decatastrophizing, Pros and Cons Analysis, Evidence-Based Questioning, Reality Testing, Continuum Technique, Changing Rules to Wishes, Behavior Experiment, Problem-Solving Skills Training, Systematic Exposure

## Example 1
[Intake form written by client]
&lt;Reason for Seeking Therapy&gt;
I</span><span class="sh">'</span><span class="s">ve been struggling with my temper, ...

[CBT technique]
Alternative Perspective

## Example 2
[Intake form written by client]
</span><span class="si">{</span><span class="n">intake_form</span><span class="si">}</span><span class="s">

[CBT technique]</span><span class="sh">'''</span>

<span class="n">cbt_tech</span> <span class="o">=</span> <span class="nf">generate_response</span><span class="p">(</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">cbt_tech_generation_prompt</span><span class="p">,</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llama3-70b-8192</span><span class="sh">"</span>
<span class="p">)</span>
<span class="n">cbt_tech</span>
</code></pre></div></div> <p>실행을 해보면, <code class="language-plaintext highlighter-rouge">Evidence-Based Questioning</code> 이라는 결과가 나온다. 즉, 이 상황에 대해 적용할 수 있는 CBT 기법이 바로 Evidence-Based Questioning이라는 것이다. llama 모델은 사전학습된 데이터가 있기에 이를 활용하여 입력된 텍스트를 분석하고, 가장 적절한 것을 선택한 것이다.</p> <ol> <li>counselor-side Simulation</li> </ol> <p>이제 구체적인 플랜을 짜보자.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 3. CBT Planning
</span>
<span class="n">cbt_planning_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'''</span><span class="s">
You are a counselor specializing in CBT techniques. Plan to counsel the patient who has completed ... 

## Example 1
[Intake form written by client]
&lt;Reason for Seeking Therapy&gt;
I</span><span class="sh">'</span><span class="s">ve been struggling with ... 

&lt;Goals for Therapy&gt;
I want to find ways to stay ... 

&lt;Cognitive Distortions Observed&gt;
All-or-nothing thinking: The client ... 

[CBT technique]
Decatastrophizing

[Counseling plan]
Decatastrophizing
1. Identify Catastrophic ... 

</span><span class="si">{</span><span class="n">intake_form</span><span class="si">}</span><span class="s">

[CBT technique]
</span><span class="si">{</span><span class="n">cbt_tech</span><span class="si">}</span><span class="s">

[Counseling sequence]
</span><span class="sh">'''</span>

<span class="n">cbt_plan</span> <span class="o">=</span> <span class="nf">generate_response</span><span class="p">(</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">cbt_planning_prompt</span><span class="p">,</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llama3-70b-8192</span><span class="sh">"</span>
<span class="p">)</span>
<span class="n">cbt_plan</span>
</code></pre></div></div> <p>아주 좋은 결과를 내준다.</p> <ol> <li>dialogue 제작</li> </ol> <p>이를 가지고 dialogue를 만들어 볼 것이다</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 4. Dialogue generation
</span>
<span class="n">dialogue_generation_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'''</span><span class="s">
Your task is to generate a multi-turn counseling dialogue between a client and a professional counselor. Generate a dialogue that incorporates the following guidelines:

# General guidelines
1. The dialogue is ...

# Guidelines for the participants
## Guidelines for the counselor</span><span class="sh">'</span><span class="s">s utterance:
1. At the start of the conversation, ...

## Guidelines for the client</span><span class="sh">'</span><span class="s">s utterance:
1. Engage authentically with the counselor</span><span class="sh">'</span><span class="s">s inquiries ... 

[Situation of the client]
</span><span class="si">{</span><span class="n">intake_form</span><span class="si">}</span><span class="s">

[Counseling plan]
</span><span class="si">{</span><span class="n">cbt_plan</span><span class="si">}</span><span class="s">

Remember that you are an independent dialogue writer and should finish the dialogue by yourself.

[Generated dialogue]
</span><span class="sh">'''</span>

<span class="n">dialogue</span> <span class="o">=</span> <span class="nf">generate_response</span><span class="p">(</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">dialogue_generation_prompt</span><span class="p">,</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llama3-70b-8192</span><span class="sh">"</span>
<span class="p">)</span>
<span class="n">dialogue</span>
</code></pre></div></div> <p>그러면 멀티턴 대화를 만들어준다.</p> <p>보면 알겠지만, 지난 번에 만들었던 generate_response 함수에 프롬포트와 모델명을 넣어서 그냥 계속 생성하는 방식이다. 예시를 한개씩 주고 있으니 one-shot이 되겠다. 어렵지 않은 과정이지만, 프롬포트를 만들 때 임상/상담 전문가가 필요하겠다.</p> <h3 id="3-파인튜닝-전-기본-작업들"> <a href="#3-파인튜닝-전-기본-작업들" class="anchor-heading" aria-labelledby="3-파인튜닝-전-기본-작업들"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> (3) 파인튜닝 전 기본 작업들 </h3> <p>모델은 LLaMa2-7b , 데이터셋은 Cactus를 사용한다 (https://huggingface.co/datasets/DLI-Lab/cactus). Cactus는 위의 과정을 계속 반복하여 생성된 데이터셋이다.</p> <p>우선 필요한 자료들을 설치해준다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">U</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">huggingface</span><span class="o">/</span><span class="n">transformers</span><span class="p">.</span><span class="n">git</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">U</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">huggingface</span><span class="o">/</span><span class="n">peft</span><span class="p">.</span><span class="n">git</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">U</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">huggingface</span><span class="o">/</span><span class="n">accelerate</span><span class="p">.</span><span class="n">git</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">trl</span> <span class="n">xformers</span> <span class="n">wandb</span> <span class="n">datasets</span> <span class="n">einops</span> <span class="n">gradio</span> <span class="n">sentencepiece</span> <span class="n">bitsandbytes</span>

<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="n">ds</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">DLI-Lab/cactus</span><span class="sh">"</span><span class="p">,</span><span class="n">split</span><span class="o">=</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>파인튜닝을 할 때 기본적으로 GPU가 많이 들기 때문에, 딱 10개 데이터만 가지고 학습을 진행해볼 것이다. <code class="language-plaintext highlighter-rouge">train_dataset</code> 한 개 한 개를 보면 아까 돌았던 사이클 (client-side, counselor-side로 만든 multi-turn dialogue) 이 잘 저장되어 있다.</p> <p>지금까지의 dialogue history가 주어졌을 때, 그걸 바탕으로 counselor이 적합한 대답을 하는 걸 학습하는 방식을 통해 상담 챗봇을 만들어본다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[:</span><span class="mi">10</span><span class="p">][</span><span class="sh">'</span><span class="s">dialogue</span><span class="sh">'</span><span class="p">]</span>
<span class="n">refined_dataset</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">dialogue</span> <span class="ow">in</span> <span class="n">train_dataset</span><span class="p">:</span>
  <span class="n">splited_dialogue</span> <span class="o">=</span> <span class="n">dialogue</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">splited_dialogue</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">dialogue_history</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">splited_dialogue</span><span class="p">[:</span><span class="n">i</span><span class="o">*</span><span class="mi">2</span><span class="o">+</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">response</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">splited_dialogue</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">2</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">refined_dataset</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">refined_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <p>refined_dataset을 보면 이제 ‘dialogue_history’와 ‘response’로 나뉘어 딕셔너리 형태로 저장되는 것을 볼 수 있다. 이런 식으로 ..</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'dialogue_history': "Counselor: Good afternoon, Brooke. Thank you for joining me today. Can you tell me a bit about what brings you to counseling?Client: Hi. I've been really anxious about going back to the animal shelter where I volunteer. I feel like the animals will hate me because they didn't remember me the last time I visited. It's been really tough.",
 'response': "Counselor: I'm sorry to hear that you've been feeling this way. It sounds like this is something that’s been troubling you for a while. Can you tell me more about what happened during your last visit to the shelter?"}
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_data_module</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="n">System_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">You are playing the role of a counselor in a psychological counseling session. Your task is to generate the next counselor utterance in the dialogue. The goal is to create a natural and engaging response that builds on the previous conversation.</span><span class="sh">"</span>
  <span class="n">User_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Counseling Dialogue History:</span><span class="se">\n</span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="sh">'</span><span class="s">dialogue_history</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span>
  <span class="n">output_text</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="sh">'</span><span class="s">response</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span>
  <span class="n">input_text</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;</span><span class="se">\n\n</span><span class="si">{</span><span class="n">System_prompt</span><span class="si">}</span><span class="s">&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</span><span class="se">\n\n</span><span class="si">{</span><span class="n">User_prompt</span><span class="si">}</span><span class="s">&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;</span><span class="se">\n\n</span><span class="sh">"</span>
  <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="n">input_text</span><span class="p">,</span> <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="n">output_text</span><span class="p">}</span>

<span class="n">transformed_dataset</span> <span class="o">=</span> <span class="p">[</span><span class="nf">make_data_module</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">refined_dataset</span><span class="p">]</span>
</code></pre></div></div> <h3 id="4-파인튜닝-진짜-해보자"> <a href="#4-파인튜닝-진짜-해보자" class="anchor-heading" aria-labelledby="4-파인튜닝-진짜-해보자"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> (4) 파인튜닝 진짜 해보자 </h3> <p>이제 진짜로 파인튜닝을 해보자. 먼저 hugging face에서 api를 받아서 오자.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>
<span class="nf">notebook_login</span><span class="p">()</span>
</code></pre></div></div> <p>실행하면 나오는 칸에 LLAMA api key를 넣어주면 된다.</p> <p>모델은 <code class="language-plaintext highlighter-rouge">Llama-2-7b-hf</code> 를 사용해보겠다 (다른 거 써도 된다.)</p> <p>LLaMA-2 모델을 4비츠 양자화와 LoRA를 사용해서 훈련하는 방식이다. (보통 LoRA는 미세조정 시 많이 사용하고 적은 리소스로 파라미터를 효율적으로 훈련할 수 있도록 한다)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">meta-llama/Llama-2-7b-hf</span><span class="sh">"</span>

<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
<span class="kn">from</span> <span class="n">peft</span> <span class="kn">import</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">,</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">PeftModel</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="nc">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span> <span class="sh">"</span><span class="s">nf4</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="p">{</span><span class="sh">""</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="nf">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="bp">False</span> <span class="c1"># silence the warnings. Please re-enable for inference!
</span><span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">pretraining_tp</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Load LLaMA tokenizer
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">add_eos_token</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">add_bos_token</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">add_eos_token</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">bnb_config</code> 는 4비트 양자화를 통해 모델의 메모리 사용량을 줄이고, nf4를 통해 양자화 유형을 정해주었으며 torch.float16으로 계산 데이터 유형을 지정하였다.</p> <p><code class="language-plaintext highlighter-rouge">AutoModelForCausalLM.from_pretrained</code> 를 통해 사전 학습된 언어 모델을 로드했고, <code class="language-plaintext highlighter-rouge">quantization_config=bnb_config</code> 를 통해 아까 설정해둔 4비트 양자화를 적용했다. <code class="language-plaintext highlighter-rouge">device_map={"": 0}</code> 을 통해 모델을 GPU에 로드하도록 했다.</p> <p><code class="language-plaintext highlighter-rouge">add_eos_token</code><strong>,</strong> <code class="language-plaintext highlighter-rouge">add_bos_token</code> 을 통해 문장의 시작과 끝을 자동으로 추가하도록 했다.</p> <p>LoRA 설정을 위해 세부 매개변수를 지정하고, TrainingArguments 클래스로 모델 훈련을 위한 하이퍼파라미터를 설정한다. 그리고 사전 학습된 모델을 LoRA 기반으로 미세 조정하는 설정을 구현한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span> 

<span class="n">peft_config</span> <span class="o">=</span> <span class="nc">LoraConfig</span><span class="p">(</span>
    <span class="n">lora_alpha</span><span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">lora_dropout</span><span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">r</span><span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="sh">"</span><span class="s">CAUSAL_LM</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">q_proj</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">k_proj</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">v_proj</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">o_proj</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">gate_proj</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">up_proj</span><span class="sh">"</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">training_arguments</span> <span class="o">=</span> <span class="nc">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span> <span class="sh">"</span><span class="s">./results</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="c1">#기본값: 8 (코랩에서 돌아가도록 하려면 1이 좋다) 
</span>    <span class="n">gradient_accumulation_steps</span><span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="sh">"</span><span class="s">paged_adamw_8bit</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span> <span class="mi">30</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span> <span class="mf">2e-4</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
    <span class="n">bf16</span><span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
    <span class="n">max_grad_norm</span><span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
    <span class="n">max_steps</span><span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">warmup_ratio</span><span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
    <span class="n">group_by_length</span><span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span> <span class="sh">"</span><span class="s">linear</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">report_to</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">]</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="n">trl</span> <span class="kn">import</span> <span class="n">SFTTrainer</span>

<span class="n">hf_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">.</span><span class="nf">from_dict</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">input</span><span class="sh">'</span><span class="p">]</span><span class="si">}{</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">transformed_dataset</span><span class="p">]</span>
<span class="p">})</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="nc">SFTTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">hf_dataset</span><span class="p">,</span>
    <span class="n">peft_config</span><span class="o">=</span><span class="n">peft_config</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_arguments</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <p>모든 설정을 끝냈으니 이제 훈련을 해볼것이다. 우리의 모든 설정을 포함하고 있는 <code class="language-plaintext highlighter-rouge">trainer</code> 을 이용한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trainer</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
</code></pre></div></div> <p>데이터 양이 적어서 한 20분이면 끝난다.</p> </main> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
