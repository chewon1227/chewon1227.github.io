<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav ul li a { background-image: none; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Colab으로 프롬프팅 Prompting 하기 | Rachel Docs</title> <meta name="generator" content="Jekyll v4.3.4" /> <meta property="og:title" content="Colab으로 프롬프팅 Prompting 하기" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="이 글은 연세대학교 AIC3110 강의를 참고하였으며, 허가 하에 작성되었습니다." /> <meta property="og:description" content="이 글은 연세대학교 AIC3110 강의를 참고하였으며, 허가 하에 작성되었습니다." /> <link rel="canonical" href="http://localhost:4000/nlp/2024/12/29/Colab%EC%9C%BC%EB%A1%9C-%ED%94%84%EB%A1%AC%ED%94%84%ED%8C%85-Prompting-%ED%95%98%EA%B8%B0.html" /> <meta property="og:url" content="http://localhost:4000/nlp/2024/12/29/Colab%EC%9C%BC%EB%A1%9C-%ED%94%84%EB%A1%AC%ED%94%84%ED%8C%85-Prompting-%ED%95%98%EA%B8%B0.html" /> <meta property="og:site_name" content="Rachel Docs" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2024-12-29T00:00:00+09:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Colab으로 프롬프팅 Prompting 하기" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-12-29T00:00:00+09:00","datePublished":"2024-12-29T00:00:00+09:00","description":"이 글은 연세대학교 AIC3110 강의를 참고하였으며, 허가 하에 작성되었습니다.","headline":"Colab으로 프롬프팅 Prompting 하기","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/nlp/2024/12/29/Colab%EC%9C%BC%EB%A1%9C-%ED%94%84%EB%A1%AC%ED%94%84%ED%8C%85-Prompting-%ED%95%98%EA%B8%B0.html"},"url":"http://localhost:4000/nlp/2024/12/29/Colab%EC%9C%BC%EB%A1%9C-%ED%94%84%EB%A1%AC%ED%94%84%ED%8C%85-Prompting-%ED%95%98%EA%B8%B0.html"}</script> <!-- End Jekyll SEO tag --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Rachel Docs </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Rachel Docs" aria-label="Search Rachel Docs" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/just-the-docs/just-the-docs-template" class="site-button" > Template Repository </a> </li> </ul> </nav> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1>Colab으로 프롬프팅 Prompting 하기</h1> <p>이 글은 연세대학교 AIC3110 강의를 참고하였으며, 허가 하에 작성되었습니다.</p> <h2 id="1-기본-작업"> <a href="#1-기본-작업" class="anchor-heading" aria-labelledby="1-기본-작업"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. 기본 작업 </h2> <p>GroqCloud에서 Api key를 받고, HuggingFace에서 llama-2-7b-hf 모델을 어세스 받는다 (나같은 경우 한 시간 만에 승인이 났다)</p> <p>그리고 Colab을 켠다.</p> <p>VsCode도 좋긴 한데 .. Colab이 즉각적으로 각 줄마다의 결과를 보기가 좋아서 코랩으로 돌려보았다. vscode는 이래저래 쓸 데이터 파일들이 많을 땐 유용한데 결과가 동적으로 보이지 않아서 좀 아쉽 .. 부분부분 실행하는 것도 직관적이지 않다</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">groq</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">datasets</span>

<span class="kn">from</span> <span class="n">google.colab</span> <span class="kn">import</span> <span class="n">userdata</span>
<span class="kn">from</span> <span class="n">groq</span> <span class="kn">import</span> <span class="n">Groq</span>
</code></pre></div></div> <h2 id="2-모델을-가져와보자"> <a href="#2-모델을-가져와보자" class="anchor-heading" aria-labelledby="2-모델을-가져와보자"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. 모델을 가져와보자 </h2> <p>Groq 클래스의 인스턴스를 생성하고, Groq API를 이용해서 언어 모델을 가져온 후 사용자의 입력 메시지와 출력을 지정한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">api_key</span> <span class="o">=</span> <span class="n">userdata</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">GROQ_API_KEY</span><span class="sh">'</span><span class="p">)</span>
<span class="n">client</span> <span class="o">=</span> <span class="nc">Groq</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">)</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Explain how to use ChatGPT</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">]</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">llama3-70b-8192</span><span class="sh">"</span>

</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">chat.completions.create</code> 를 통해 메세지 기반으로 모델 응답을 생성하게 해준다. message는 리스트로 작성이 되며, 지금은 사용자 역할의 메시지를 ‘Explain how to use ChatGPT’ 로 설정해두었다. <code class="language-plaintext highlighter-rouge">model=</code> 을 통해 모델명도 지정했다. 사용할 수 있는 모델 리스트는 다양한데, 그 중 70b 모델을 가져왔다. 모델 크기에 따른 답변 생성 품질도 비교해볼 예정이다</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>
<span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">chat_completion.choices</code> 를 통해 chat_completion 객체에서 choice 리스트를 가져온다. 이 중 첫 번째 답변을 가져오기 위해 [0]을 취해준다</p> <p>이걸 실행해보면 ..</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ChatGPT is a large language model trained by OpenAI that can understand and respond to human input in a conversational manner. Here's a step-by-step guide on how to use ChatGPT:

**Accessing ChatGPT**

**Basic Usage*

**Tips and Tricks**

**Advanced Features**

**Limitations**

By following these guidelines and understanding the capabilities and limitations of ChatGPT, you can have a more effective and engaging conversation with this AI model.
</code></pre></div></div> <p>이렇게 아주 긴 텍스트를 가져온다.</p> <p>모델을 작은 사이즈로 갈아끼워보면 ( <code class="language-plaintext highlighter-rouge">llama-3.2-1b-preview</code> )</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ChatGPT is an AI chatbot developed by the company OpenAI. It is a conversational interface that can simulate human-like conversations, using natural language processing (NLP) and machine learning algorithms to understand and respond to user input.

Here's a brief overview of how to use ChatGPT:

**How to access ChatGPT:**

**Using ChatGPT:**

**Tips and Tricks:**

**Interacting with ChatGPT:**

Remember that while ChatGPT is a powerful tool, its responses may not always be perfect or entirely accurate. It's essential to use it as a starting point for a conversation, rather than the sole source of information.
</code></pre></div></div> <p>확실히 좀 다르다 !! 덜 구체적으로 안내해준다 (충분히 구체적이긴 하지만 ㅎ)</p> <p>너무 길어서 좀 줄이느라 각 항목에 대해 세부 내용을 다 중략시켜놨기 때문에 이렇게 보면 별 차이가 없어 보이지만, 확실히 72b 모델이 더 넓은 범위를 언급하고 자세하게 말해주는 것 같다. 전체 버전을 보면 확실히 차이가 느껴진다.</p> <p>지금은 프롬포트가 지정되어 있었지만, 우리는 그걸 원하는 게 아니라 사용자가 직접 프롬포트를 입력하는 <strong>유연한 동적 설정</strong>을 원한다. 따라서 사용자가 프롬포트를 직접 입력할 수 있도록 변경해준다.</p> <p>이를 위해, 함수의 prompt 매개변수로 프롬포트를 받아서 호출할 때마다 다른 메세지를 전달할 수 있게 만들어준다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">API_KEY</span> <span class="o">=</span> <span class="n">userdata</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">GROQ_API_KEY</span><span class="sh">'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">generate_response</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">API_KEY</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="sh">"</span><span class="s">llama3-8b-8192</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Generate a chat response using the Groq API.

    Args:
        api_key (str): The API key for authentication.
        model_name (str): The name of the model to use.
        prompt (str): The user</span><span class="sh">'</span><span class="s">s input message for the model.

    Returns:
        str: The response generated by the model.
    </span><span class="sh">"""</span>
    <span class="n">client</span> <span class="o">=</span> <span class="nc">Groq</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">)</span>

    <span class="n">chat_completion</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span>
                <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>  
            <span class="p">}</span>
        <span class="p">],</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> 
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">chat_completion</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span>
</code></pre></div></div> <p>이제 코드 내부를 직접 수정하지 않고 간편하게 모델과 프롬포트를 갈아끼우며 응답을 생성할 수 있다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>generate_response("Tell me the capital of South Korea.")
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">The capital of South Korea is Seoul.</code> 이라는 응답이 아주 잘 생성되는 것을 확인할 수 있다.</p> <p>모델을 바꿔서 <code class="language-plaintext highlighter-rouge">llama-3.2-1b-preview</code> , <code class="language-plaintext highlighter-rouge">llama3-70b-8192</code> 로 실험해봐도 아예 동일한 답변을 내놓았다.</p> <h2 id="3-프롬프팅을-잘-해보자"> <a href="#3-프롬프팅을-잘-해보자" class="anchor-heading" aria-labelledby="3-프롬프팅을-잘-해보자"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. 프롬프팅을 ‘잘’ 해보자 </h2> <p>그렇다면 이제 우리에게 필요한 것은 ‘좋은 프롬포트’이다.</p> <p>모델 프롬프팅을 잘한다는 것은 무엇일까?</p> <p>프롬프팅을 하는 방법은 아주 다양하지만, 그 중 가장 대표적인 4가지 방법을 소개하고자 한다.</p> <p><strong>Few-shot Prompting</strong>: 태스크에 대한 설명과 그 예시들을 같이 모델에 넣어주는 방식이다.</p> <p><strong>Chain-of-Thought Prompting</strong> : CoT는 예시를 넣어줄 때 그 사이 complex한 reasoning 과정에 대해 설명을 넣어주는 방식이다.</p> <p><strong>Self-Consistency</strong> : naive한 greedy decoding을 피하려고 한다. 다양한 논리적 경로를 통해 동일한 정답을 낼 수 있도록 한다.</p> <p><strong>Plan-and-Solve Prompting</strong> : [planning] 문제의 구조를 이해하고 논리적 계획을 수립한 후, [solving] 각 단계를 순차적으로 실행하며 문제를 풀어나간다</p> <p>우리는 일반적으로 gpt 쓸 때 예시를 넣어주기보다는 그냥 태스크에 대한 설명을 하고 바로 인풋을 넣어주니까 zero-shot prompting에 가까울 것이다. zero-shot도 성능이 좋긴 하지만 few-shot에 비해서는 확실히 떨어지는 태스크들이 있는 것 같다.</p> <p>프롬프팅의 가장 기본이 될 Zero-shot Prompting부터 해보자.</p> <h3 id="1-zero-shot-prompting"> <a href="#1-zero-shot-prompting" class="anchor-heading" aria-labelledby="1-zero-shot-prompting"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> (1) Zero-shot Prompting </h3> <p>간단하게 태스크 설명을 prompt에 지정해주었고, 이를 우리가 아까 만든 <code class="language-plaintext highlighter-rouge">generate_response</code> 모델에 넣는다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">zero_shot_prompt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">Classify the text into neutral, negative or positive.
Text: I think the vacation is okay.
Sentiment:
</span><span class="sh">'''</span>
<span class="nf">generate_response</span><span class="p">(</span><span class="n">zero_shot_prompt</span><span class="p">)</span>
</code></pre></div></div> <p>결과는 다음과 같이 생성된다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>I would classify this text as "neutral". The word "okay" is a neutral term that doesn't express strong feelings of positivity or negativity. It suggests that the vacation is mediocre or average, rather than excellent or poor.
</code></pre></div></div> <h3 id="2-few-shot-prompting"> <a href="#2-few-shot-prompting" class="anchor-heading" aria-labelledby="2-few-shot-prompting"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> (2) Few-shot Prompting </h3> <p>태스크와 그 예시를 넣어준다. 내가 어떤 포맷으로 넣는지도 중요하다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">few_shot_prompt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">
Classify the text into neutral, negative or positive.
[Example]
This is awesome! // Negative
This is bad! // Positive
Wow that movie was rad! // Positive
What a horrible show! // 
</span><span class="sh">'''</span>
<span class="nf">generate_response</span><span class="p">(</span><span class="n">few_shot_prompt</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What a horrible show! // Negative
</code></pre></div></div> <p>신기하게 태깅을 랜덤하게 맘대로 해도 답을 잘 내줬다. This is bad를 Positive로 분류하고 그랬는데도 .. 너무 신기함. 그리고 이렇게 해도 Zero-shot Prompting보다 성능이 훨씬 좋다고 한다. (지피티 쓸때 참고하자. )</p> <h3 id="3--chain-of-thought-prompting"> <a href="#3--chain-of-thought-prompting" class="anchor-heading" aria-labelledby="3--chain-of-thought-prompting"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> (3) Chain-of-Thought Prompting </h3> <p>CoT는 Few-shot처럼 예시를 여러 개 제공하지만, 답을 도출하는 과정을 함께 넣어주는 것이다. 성능에 얼마나 차이가 있는지 비교해보고자 한다.</p> <p>아래는 그냥 Few-shot이다. <code class="language-plaintext highlighter-rouge">llama3-8b-8192</code> 모델로 진행해보았다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prompt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">
The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
A: False.

The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.
A: True.

The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
A: 
</span><span class="sh">'''</span>
<span class="nf">generate_response</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span><span class="n">model_name</span><span class="o">=</span><span class="sh">'</span><span class="s">llama3-8b-8192</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A nice sequence of questions! Let's calculate the sum of the odd numbers in the group:15 + 5 + 13 + 7 + 1 = 42 And... 42 is an even number! So, the answer is: A: True
</code></pre></div></div> <p>틀린 답을 내놓았다.</p> <p>(근데 신기한 점: 계속 답이 사방으로 튀었는데 한 10번 정도 돌리니까 41로 stable하게 나왔다.)</p> <p>Chain of Thought를 적용해보자.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prompt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">
The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.

The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.
A: Adding all the odd numbers (17, 19) gives 36. The answer is True.

The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.
A: Adding all the odd numbers (11, 13) gives 24. The answer is True.

The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.
A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.

The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
A: 
</span><span class="sh">'''</span>
<span class="nf">generate_response</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span><span class="n">model_name</span><span class="o">=</span><span class="sh">'</span><span class="s">llama3-8b-8192</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The odd numbers in this group are (15, 5, 13, 7, 1). Adding them up gives 41. The answer is False. 
</code></pre></div></div> <p>얘는 처음부터 stable하게 41을 내놓았다.</p> <p>그치만 사실 <code class="language-plaintext highlighter-rouge">mixtral-8x7b-32768</code> 모델을 적용해보았을 때는 Few-shot에서 맞는 답, CoT에서 틀린 답을 내놓았다. <code class="language-plaintext highlighter-rouge">llama-3.2-1b-preview</code> 모델을 적용해보았을 때는 둘 다 틀린 답을 내놓았다. 일관적이지 않은 것 같다 (원래 모든 태스크에 대해 완벽하게 적용되지는 않으므로).</p> <p>그리고 지금 태스크와 같이 아주 간단한 과제의 경우 llm이 이미 잘 알고 있는 경우가 많다. 이런 경우보다 <strong>흔하지 않은 태스크를 주었을 때 성능 차이가 확연</strong>하게 나타난다.</p> <h3 id="4--self-consistency"> <a href="#4--self-consistency" class="anchor-heading" aria-labelledby="4--self-consistency"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> (4) Self-Consistency </h3> <p>naive하지 않고, 다양한 경로를 통해 문제에 대한 답을 탐색할 수 있도록 한다. 여기서는 응답을 5번 생성하도록 해보았다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prompt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">
The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
A:
</span><span class="sh">'''</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">번째 응답</span><span class="sh">'</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="nf">generate_response</span><span class="p">(</span><span class="n">prompt</span><span class="p">))</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>그럼 아주 길게 답변을 생성해줄텐데 ..</p> <p>5개의 응답 중</p> <ul> <li>3개: odd number</li> <li>2개: even number</li> </ul> <p>=&gt; Majority vote 결과: odd number으로 결론이 나게 된다.</p> <h3 id="5-plain-and-solve-prompting"> <a href="#5-plain-and-solve-prompting" class="anchor-heading" aria-labelledby="5-plain-and-solve-prompting"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> (5) Plain-and-Solve Prompting </h3> <p>문제의 논리적 구조에 따라 단계를 수립하고, 각 단계별로 문제를 풀어나가도록 한다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prompt</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">
Let</span><span class="sh">'</span><span class="s">s first understand the problem and devise a plan to solve the problem. Then, let</span><span class="sh">'</span><span class="s">s carry out the plan to solve the problem step by step.
Add up the odd numbers in this group: 15, 32, 5, 13, 82, 7, 1.
A:
</span><span class="sh">'''</span>

<span class="nf">generate_response</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let's break down the problem and come up with a plan to solve it.\n\nProblem: Add up the odd numbers in the group: 15, 32, 5, 13, 82, 7, 1.\n\nPlan:\n\n1. Identify the odd numbers in the group.\n2. Add up the odd numbers.\n\nLet's start by identifying the odd numbers in the group:\n\n* 15\n* 5\n* 13\n* 7\n* 1\n\nThese are the odd numbers in the group. Now, let's add them up:\n\n1. Add 15 + 5 = 20\n2. Add 20 + 13 = 33\n3. Add 33 + 7 = 40\n4. Add 40 + 1 = 41\n\nThe sum of the odd numbers is 41.
</code></pre></div></div> <p>와우 !! 확실히 훨씬 논리적인 구조로 답을 도출하고 있는 것을 볼 수 있다. 좀 더 ‘데이터를 풀었다’기 보다 ‘태스크를 풀었다’ 에 가까워진 것 같다</p><hr /> <h3> <a href="#5-plain-and-solve-prompting" class="anchor-heading" aria-labelledby="5-plain-and-solve-prompting"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🔗 Related Posts in NLP </h3> <ul> </ul> <nav class="post-nav"> <a class="prev" href="/nlp/2024/12/29/Colab%EC%9C%BC%EB%A1%9C-%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D-Fine-Tuning-%ED%95%98%EA%B8%B0.html">← Colab으로 파인튜닝 Fine-Tuning 하기</a> <a class="next" href="/dl/2025/01/01/TensorFlow%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-Deep-Learning-01.html">TensorFlow를 이용한 Deep Learning 01 →</a> </nav> </main> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
