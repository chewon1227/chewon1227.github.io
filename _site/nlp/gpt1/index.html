<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav ul li a { background-image: none; } .site-nav > ul.nav-category-list > li > button svg { transform: rotate(-90deg); } .site-nav > ul.nav-category-list > li.nav-list-item > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>[GPT-1] Improving Language Understanding by Generative Pre-Training(2018) | Rachel Docs</title> <meta name="generator" content="Jekyll v4.3.4" /> <meta property="og:title" content="[GPT-1] Improving Language Understanding by Generative Pre-Training(2018)" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="chasing questions - psychology and ai" /> <meta property="og:description" content="chasing questions - psychology and ai" /> <link rel="canonical" href="http://localhost:4000/nlp/gpt1/" /> <meta property="og:url" content="http://localhost:4000/nlp/gpt1/" /> <meta property="og:site_name" content="Rachel Docs" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2025-04-15T00:00:00+09:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="[GPT-1] Improving Language Understanding by Generative Pre-Training(2018)" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-04-15T00:00:00+09:00","datePublished":"2025-04-15T00:00:00+09:00","description":"chasing questions - psychology and ai","headline":"[GPT-1] Improving Language Understanding by Generative Pre-Training(2018)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/nlp/gpt1/"},"url":"http://localhost:4000/nlp/gpt1/"}</script> <!-- End Jekyll SEO tag --> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <!-- KaTeX for math rendering --> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, { delimiters: [ {left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false} ] });"> </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Rachel Docs </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"> <li class="nav-list-item"> <a href="/" class="nav-list-link">Home</a> </li> <li class="nav-list-item"> <a href="/categories/dl.html" class="nav-list-link">DL</a> </li> <li class="nav-list-item"> <a href="/categories/nlp.html" class="nav-list-link">NLP</a> </li> <li class="nav-list-item"> <a href="/categories/eda.html" class="nav-list-link">EDA</a> </li> <li class="nav-list-item"> <a href="/categories/paper.html" class="nav-list-link">PAPER</a> </li> </ul> </nav> <nav aria-label="Categories" id="category-nav" class="site-nav"> <ul class="nav-list nav-category-list"> <li class="nav-list-item"> <button class="nav-list-expander btn-reset" aria-label="Toggle category DL" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <div class="nav-category">DL</div> <ul class="nav-list"> <li class="nav-list-item"> <a href="/dl/tensorflow-04/" class="nav-list-link"> TensorFlow를 이용한 Deep Learning 04 </a> </li> <li class="nav-list-item"> <a href="/dl/tensorflow-03/" class="nav-list-link"> TensorFlow를 이용한 Deep Learning 03 </a> </li> <li class="nav-list-item"> <a href="/dl/tensorflow-02/" class="nav-list-link"> TensorFlow를 이용한 Deep Learning 02 </a> </li> <li class="nav-list-item"> <a href="/dl/tensorflow-01/" class="nav-list-link"> TensorFlow를 이용한 Deep Learning 01 </a> </li> </ul> </li> </ul> <ul class="nav-list nav-category-list"> <li class="nav-list-item"> <button class="nav-list-expander btn-reset" aria-label="Toggle category EDA" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <div class="nav-category">EDA</div> <ul class="nav-list"> <li class="nav-list-item"> <a href="/eda/bus-eda-01/" class="nav-list-link"> 서울시 버스 노선 정류장 승하차 분석 - 01 </a> </li> </ul> </li> </ul> <ul class="nav-list nav-category-list"> <li class="nav-list-item"> <button class="nav-list-expander btn-reset" aria-label="Toggle category NLP" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <div class="nav-category">NLP</div> <ul class="nav-list"> <li class="nav-list-item"> <a href="/nlp/react/" class="nav-list-link"> ReAct: Synergizing Reasoning and Acting in Language Models </a> </li> <li class="nav-list-item"> <a href="/nlp/gpt1/" class="nav-list-link"> [GPT-1] Improving Language Understanding by Generative Pre-Training(2018) </a> </li> <li class="nav-list-item"> <a href="/nlp/batchapi/" class="nav-list-link"> 일반 API와 Batch API 사용하기 </a> </li> <li class="nav-list-item"> <a href="/nlp/colab-finetune/" class="nav-list-link"> Colab으로 파인튜닝(Fine-Tuning)하기 </a> </li> <li class="nav-list-item"> <a href="/nlp/colab-prompting/" class="nav-list-link"> Colab으로 프롬프팅(Prompting)하기 </a> </li> </ul> </li> </ul> <ul class="nav-list nav-category-list"> <li class="nav-list-item"> <button class="nav-list-expander btn-reset" aria-label="Toggle category Paper" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <div class="nav-category">Paper</div> <ul class="nav-list"> <li class="nav-list-item"> <a href="/paper/2025/03/29/Attention.html" class="nav-list-link"> [Attention] Neural Machine Translation by Jointly Learning to Align and Translate (2015) </a> </li> <li class="nav-list-item"> <a href="/nlp/seq2seq/" class="nav-list-link"> Sequence-to-Sequence Models </a> </li> <li class="nav-list-item"> <a href="/paper/2025/02/24/LSTM.html" class="nav-list-link"> [LSTM] Long Short Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling (2014) </a> </li> <li class="nav-list-item"> <a href="/paper/2025/02/14/Word2Vec.html" class="nav-list-link"> [Word2Vec] Efficient Estimation of Word Representations in Vector Space (2013) </a> </li> <li class="nav-list-item"> <a href="/paper/2025/02/05/RNN.html" class="nav-list-link"> [RNN] Recurrent neural network based language model (2010) </a> </li> </ul> </li> </ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Rachel Docs" aria-label="Search Rachel Docs" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/just-the-docs/just-the-docs-template" class="site-button" > Template Repository </a> </li> </ul> </nav> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1>[GPT-1] Improving Language Understanding by Generative Pre-Training(2018)</h1> <p><br /></p> <p><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a></p> <p>트랜스포머 논문보다 훨씬 친절하다.</p> <p><br /></p> <h2 id="1-intro"> <a href="#1-intro" class="anchor-heading" aria-labelledby="1-intro"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. Intro </h2> <p>모델을 학습시킬 때 가장 많은 시간이 소요되는 것은 바로 data annotation, data curation인 것 같다. 실제로 아주 정제가 잘 되어 가공할 필요가 거의 없는 데이터란 존재하지 않고, 가공되어있다 한들 연구자들이 사용하려고 하는 라벨 / 분류와 정확히 일치할 수가 없기 때문이다. 가공되어 있어도 이정도인데, 보통 연구에서 사용하려고 하는 real data는 비가공 데이터로 전혀 정제되어 있지 않고, 형식조차 중구난방이다.</p> <p>이 논문에서도 이 부분을 supervised NLP에서의 한계점으로 찝으며 논의를 시작하고 있다. labeled data를 사용하기 위해서는 결국 annotation이 필요하며, 이를 하다가 끝난다는 것이다.</p> <div class="callout"> 아니 그러면 unlabeled data로 해보면 되는거 아니야? 그게 어려운 이유는 다음과 같다. - text representation을 학습할 때 가장 좋은 것이 뭔지 불확실하다. - 학습된 표현을 target task로 연결시킬 때, 합의된 가장 효과적인 방법이 없다. </div> <p>그래서 여기선 <code class="language-plaintext highlighter-rouge">semi-supervised approach</code> 를 제시한다. 즉, unsupervised pre-training과 supervised fine-tuning을 결합하는 것이다. 광범위한 task에 최소한의 adaption을 가지고도 커버할 수 있는 모델을 구현하는 것이 최종적인 목적이라고 할 수 있다. 이를 위해 두 단계의 학습을 수행한다.</p> <ol> <li>unlabeld data에 대해서 language modeling objective를 사용하여 initial parameter를 학습함</li> <li>이를 target task에 적용하기 위해서 supervised learning</li> </ol> <p>모델의 구성 요소로는 transformer을 활용한다. transformer은 attention 구조를 활용하기에 long-term text와 diverse task에 transfer할 때 강력하기 때문이다. transfer을 할 때 <code class="language-plaintext highlighter-rouge">traversal-style approaches</code> 를 기반으로 하여, text input을 single contiguous sequence of token들로 구조화하는 작업을 진행한다. Pre-train된 모델에서 변형을 최소화하여 파인튜닝을 효과적으로 진행할 수 있다.</p> <div class="callout"> Traversal-style approach가 뭔데? task가 자연어로 설명되어 입력될 때 그 입력을 토큰으로 구분하는 것. 보통 start token, end token, delimiter 등으로 이루어져 있다. </div> <p><br /></p> <h2 id="2-related-work"> <a href="#2-related-work" class="anchor-heading" aria-labelledby="2-related-work"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. Related Work </h2> <h3 id="2-1-semi-supervised-learning-for-nlp"> <a href="#2-1-semi-supervised-learning-for-nlp" class="anchor-heading" aria-labelledby="2-1-semi-supervised-learning-for-nlp"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2-1. Semi-supervised Learning for NLP </h3> <p>초기에는 unlabeled data를 이용하여 word-level / phrase-level 을 계산한 후 이를 supervised model의 특성으로 이용하였다. 그리고 실제로 연구자들은 unlabeled copora에서 학습된 word embedding을 이용하여 성능을 개선해왔다. 그치만, 이들은 그저 word-level에서만 작동할 뿐, 우리가 주목하고자 하는 long-term과는 맞지 않는다. 그래서 최근 연구자들은 unlabeled 데이터에서 word-level 이상을 학습할 수 있도록 하는 인코딩을 목표로 연구하고 있다.</p> <h3 id="2-2-unsupervised-pre-training"> <a href="#2-2-unsupervised-pre-training" class="anchor-heading" aria-labelledby="2-2-unsupervised-pre-training"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2-2. Unsupervised Pre-training </h3> <p>unsupervised pre-trainin은 good initialization point를 잡는 것에 가장 큰 초점을 두고 있다 (supervised learning objective 수정하기 x). 각 task에 대해 모두 맞춤형으로 제작하는 것이 아니라, 하나의 모델이 최소한의 변경만 가지고 adapt할 수 있도록 하여 효과성을 극대화한다.</p> <p><br /></p> <h2 id="3-framework"> <a href="#3-framework" class="anchor-heading" aria-labelledby="3-framework"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. Framework </h2> <h3 id="3-1-unsupervised-pre-training"> <a href="#3-1-unsupervised-pre-training" class="anchor-heading" aria-labelledby="3-1-unsupervised-pre-training"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3-1. Unsupervised Pre-training </h3> <p>Unlabeled 된 token $ U = {u_1, …, u_n} $가 주어졌을 때, MLE를 위해</p> <p>$ L_1(U) = \sum_{i} \log P(u_i | u_{i-k}, …, u_{i-1}; \Theta) \quad (1) $</p> <p>를 사용한다.</p> <p>k는 context window size이고, P는 conditional probability이다. 즉, <strong>k개의 문맥 단위 창문으로 그 다음에 u_i가 나올 확률이 maximize되도록 \Theta 를 학습한다</strong> , 라고 이해하면 될 듯 하다. i 번째 단어를 위해 (i-k)번째부터 (i-1)번째 단어까지를 보고, i 번째 단어가 나올 확률을 최대화하도록 한다는 것이다. MLE로 적용되어 loss function 기반으로 학습하게 될 것이다. SGD를 활용하기에 back-propagation도 진행한다.</p> <p>Language Model로는 multi-layer Transformer decoder을 이용하여 multi-headed self-attention을 진행한 후, position-wise feed-forward layer을 통해 target token에 대한 output distribution을 만든다. decoder만 이용한다는 점이 특징이다.</p> <p>$ h_0 = U W_e + W_p h_l = transformer*block(h*{l-1}) \quad \forall i \in [1, n] $</p> <p>$ P(u) = softmax(h_n W^T_e) \quad<br /> $</p> <p>$ U = (u_{-k}, …, u_{-1}) $ 는 토큰의 context vector은 layer 수이다. 즉, context vector * token embedding + positional embedding으로 h0을 만들어서 transformer_block에 넣고, 그거랑 token embedding의 전치행렬을 곱한 것(점곱)을 softmax에 넣어서 확률값을 구한다. 이렇게 output distribution을 구한다는 것이다.</p> <h3 id="3-2-supervised-fine-tuning"> <a href="#3-2-supervised-fine-tuning" class="anchor-heading" aria-labelledby="3-2-supervised-fine-tuning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3-2. Supervised fine-tuning </h3> <p>이렇게 initialization을 진행한 후, labeled dataset을 이용하여 parameter들을 supervised target task에 맞게 조정한다. input $ x_1, …, x_m $과 그에 대한 label y가 있다고 생각할 때, input이 pre-trained model을 통과하며 activation $ h^m_l $를 얻고, W_y parameter과 함께 added linear output layer에 들어가서 최종적으로 y를 예측하게 된다.</p> <p>$ P(y | x_1, …, x_m) = softmax(h^m_l W_y) \quad<br /> $</p> <p>즉, x_1부터 x_m 까지의 입력값을 가지고 activation과 parameter을 곱해서 softmax를 취하여, x_1 ~ x_m 이 입력값으로 들어갔을 때 y 가 도출될 확률을 구하는 것이다.</p> <p>이후, 이를 모든 input-label에 대해 진행하여, 로그합을 구해 L_2(C)를 구한다.</p> <p>$ L_2(C) = \sum_{(x,y)} \log P(y | x_1, …, x_m) \quad $</p> <p>직관적으로 생각하면, 모델이 y에 대해서 높은 확률을 예측했다면 L_2(C)가 큰 값을 가질 것이다. 학습 시 이렇게 최대화하는 방향으로 파라미터가 조정될 것이고, 이는 모델이 target task에 대해 높은 정확도로 예측을 한다는 의미이다. 즉, Loss Function이다.</p> <p>추가적으로 fine-tuning을 할 때, language modeling을 auxiliary objective로 추가하는 것이 좋다는 것을 발견했다. 그 이유는 다음과 같다.</p> <ol> <li>supervised model의 generalization 능력을 향상시킨다</li> <li>convergence를 가속화한다 (빠르게 안정화된다는 뜻)</li> </ol> <p>그래서 unsupervised learning을 할 때 썼던 L_1(C)와 supervised fine-tuning할 때 만든 L_2(C)를 모두 사용한다 (L_1(C)에는 가중치를 붙인다).</p> <p>$ L_3(C) = L_2(C) + \lambda \cdot L_1(C) \quad (5) $</p> <div class="callout"> L_1(C)에 가중치를 붙이는 이유? 모델 성능을 최적화하기 위함. L2는 이미 task에 최적화가 되어 있으므로, general하게 initialized 되어 있는 상태인 L1을 조정해서 효율성을 높인다. 두 L function의 균형을 조절하면서, 모델이 특정한 과제에 치우치지 않도록 하는 것이다. (그냥 가중치 가해서 더하기만 한다니 .. 직관적이면서도 이로 적합이 되는 것도 참 신기 ..) </div> <p><br /></p> <h3 id="33-task-specific-input-transformations"> <a href="#33-task-specific-input-transformations" class="anchor-heading" aria-labelledby="33-task-specific-input-transformations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3.3 Task-Specific input Transformations </h3> <p>text classification과 같은 태스크에 대해서는 우리가 바로 fine-tuning하면 잘 수행해낼 수 있다. 근데 QA(질문에 답하기)나 TE(추론하기)는 구조화된 input이 있고(문장 짝, 문서, 질문-답), 우리의 pre-trained model은 아예 contiguous sequence 로 이루어져 있기 때문에, 이런 태스크까지 잘 수행해내기 위해서는 좀 수정이 필요할 것이다.</p> <p><strong>Textual entailment Task</strong> 를 위해서, premise <em>p</em>와 hypothesis <em>h</em> 토큰 순서를 결합한다 (사이에 delimiter token 추가)</p> <p><strong>Similarity Task</strong> 를 위해서, 두 문장의 쌍 순서가 정확하게 정해져 있는게 아니기 때문에 delimiter token을 추가한 뒤 각각 독립적으로 처리해서 두 개의 시퀀스 표현을 만든다.</p> <p><strong>Question Answering and Commensense Reasoning Task</strong> 를 위해서, context document z, question q, 가능한 answer들 {a_k}를 가지고 결합한 뒤, softmax를 통해 output 분포를 계산한다.</p><hr /> <h3> <a href="#33-task-specific-input-transformations" class="anchor-heading" aria-labelledby="33-task-specific-input-transformations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🔗 Related Posts in NLP </h3> <ul> <li><a href="/nlp/react/">ReAct: Synergizing Reasoning and Acting in Language Models</a> (2025-06-08)</li> <li><a href="/nlp/batchapi/">일반 API와 Batch API 사용하기</a> (2025-04-01)</li> <li><a href="/nlp/colab-prompting/">Colab으로 프롬프팅(Prompting)하기</a> (2024-12-29)</li> <li><a href="/nlp/colab-finetune/">Colab으로 파인튜닝(Fine-Tuning)하기</a> (2024-12-29)</li> </ul> <nav class="post-nav"> <a class="prev" href="/nlp/batchapi/">← 일반 API와 Batch API 사용하기</a> <a class="next" href="/nlp/react/">ReAct: Synergizing Reasoning and Acting in Language Models →</a> </nav> </main> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
