<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav ul li a { background-image: none; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>[RNN] Recurrent neural network based language model (2010) | Rachel Docs</title> <meta name="generator" content="Jekyll v4.3.4" /> <meta property="og:title" content="[RNN] Recurrent neural network based language model (2010)" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Intro" /> <meta property="og:description" content="Intro" /> <link rel="canonical" href="http://localhost:4000/paper/2025/01/03/RNN.html" /> <meta property="og:url" content="http://localhost:4000/paper/2025/01/03/RNN.html" /> <meta property="og:site_name" content="Rachel Docs" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2025-01-03T00:00:00+09:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="[RNN] Recurrent neural network based language model (2010)" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-01-03T00:00:00+09:00","datePublished":"2025-01-03T00:00:00+09:00","description":"Intro","headline":"[RNN] Recurrent neural network based language model (2010)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/paper/2025/01/03/RNN.html"},"url":"http://localhost:4000/paper/2025/01/03/RNN.html"}</script> <!-- End Jekyll SEO tag --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Rachel Docs </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/posts" class="nav-list-link">Posts by Category</a></li><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Rachel Docs" aria-label="Search Rachel Docs" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/just-the-docs/just-the-docs-template" class="site-button" > Template Repository </a> </li> </ul> </nav> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1>[RNN] Recurrent neural network based language model (2010)</h1> <h2 id="intro"> <a href="#intro" class="anchor-heading" aria-labelledby="intro"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Intro </h2> <p>sentence를 구성하고 단락을 구성하기 위해서는 기본적으로 단어들의 sequence가 중요하다. Statistics Language Model은 주어진 맥락에서 다음 단어를 예측하기 위한 것이다. 이를 위해서 이런 저런 시도들이 있었다.</p> <ol> <li>N-gram : 특정 순서로 인접한 n개의 기호의 시퀀스이다.</li> <li>Cache models and class-based models : 긴 맥락의 정보를 묘사하고, 그 단어들 사이에 parameter을 공유해서 나름 개선이 된다. 그치만 long-term dependencies를 stochastic GD에 의존해서 학습을 진행하기에는 한계가 있다.</li> </ol> <h2 id="model-description"> <a href="#model-description" class="anchor-heading" aria-labelledby="model-description"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Model Description </h2> <h3 id="구조"> <a href="#구조" class="anchor-heading" aria-labelledby="구조"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 구조 </h3> <p>simple RNN이라고 불리는 모델을 만들었다.</p> <p>구성 요소는 input layer x, hidden layer s, output layer y 이다. input은 모두 시간과 관련된 함수로 표현된다 (문맥은 시계열 데이터이기 때문.) input vector x 는, 현재 단어를 의미하는 vector w와 이전 레이어의 output을 concatenate해서 만들어진다 (그냥 .. 더한다는 것).</p> <p>[ x(t) = w(t) + s(t - 1) \tag{1} ]</p> <p>hidden layer에서는 다음과 같은 계산이 이루어진다.</p> <p>[ s_j(t) = f\left(\sum_i x_i(t) u_{ji}\right) \tag{2} ]</p> <p>여기서 f는 시그모이드 함수이다. hidden layer에서의 출력은 보통 확률값을 나타내는데, 시그모이드를 통해 미세한 변화도 확률값으로 변환해서 사용할 수 있다.</p> <p>그리고 최종적으로 y에서는 이런 계산이 이루어진다.</p> <p>[ y_k(t) = g\left(\sum_j s_j(t) v_{kj}\right) \tag{3} ]</p> <p>여기서 g는 소프트맥스 함수이다. softmax를 통해 output의 확률분포를 얻을 수 있다.</p> <h3 id="학습하기"> <a href="#학습하기" class="anchor-heading" aria-labelledby="학습하기"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 학습하기 </h3> <p>학습할 데이터가 많을 경우 초기값은 중요하지 않다. 일반적으로 벡터 x의 크기는 단어 V의 크기 (30,000 ~ 200,000) 와 context layer의 크기를 더한 값이다. hidden layer의 크기는 보통 30~500 unit이다 (훈련 데이터의 양을 반영해야 함).</p> <p>학습은 standard back-propagation과 SGD로 진행된다.</p> <ul> <li>초기 learning rate는 0.1로 시작하는데, 매 epoch마다 훈련된 모델에 대해 validation data로 테스트를 진행한다. validation data의 log-likelihood가 증가할 경우 새로운 epoch에서 학습을 계속한다 (log-likelihood가 증가한다 = 더 좋은 성능을 낼 수 있도록 학습될 여지가 있다).</li> <li>어떤 중요한 변화가 관찰되지 않기 시작하면, learning rate를 매 epoch에서 절반으로 낮춘다.</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>learning rate가 낮다는 게 무슨 뜻?

모델이 학습을 진행할 때에는 loss function이 가장 낮은 지점을 찾아간다는 것이다. 그 가장 낮은 지점으로 이동하기 위해 learning rate에 따라 이동하며 찾아나간다고 생각하면 되는데, learning rate가 클 경우 한 번에 이동하는 거리가 크다, 즉 보폭이 크다는 것이다. learning rate가 작을수록 점점 더 미세하게 찾아나간다는 의미이다. 
</code></pre></div></div> <ul> <li>그리고 더이상의 중요한 변화가 관찰되지 않으면 학습이 끝난다. 보통 10-20 epoch에서 달성된다.</li> </ul> <h3 id="결과와-업데이트"> <a href="#결과와-업데이트" class="anchor-heading" aria-labelledby="결과와-업데이트"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 결과와 업데이트 </h3> <p>아주 큰 사이즈의 hidden layer을 사용하더라도 overtrain이 발생하지 않는다고 한다.</p> <p>output layer은 이전 단어와 그 때까지의 맥락을 토대로 다음 단어의 확률 분포를 나타내는 형식이며, error vector은 엔트로피 기반이고 weight는 back-propagation으로 계산된다.</p> <ul> <li>error vector : desired(t) - y(t)</li> </ul> <p>desired(t)는 1-of-N coding을 기반으로 만들어진 벡터이다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1-of-N coding이란?

데이터셋을 단어별로 쪼개서 확률처럼 만들어둔 벡터이다. 정답 단어의 확률을 1로, 나머지는 0으로 설정하여 network가 예측한 확률 분포 (y(t))와 정답 분포 간의 차이를 효과적으로 계산할 수 있게 한다. 
</code></pre></div></div> <p>일반적인 Statistics Language Model에서 훈련 단계와 검증 단계에서는, 테스트 데이터로 확인하는 동안 모델이 업데이트되지 않는다. 그러나 이 모델은 <code class="language-plaintext highlighter-rouge">dynamic model</code>로, 검증 단계에서도 모델이 새로운 데이터를 처리하면서 가중치가 업데이트된다.</p> <p>dynamic model이 필요한 이유는 다음과 같다.</p> <ol> <li>새로운 데이터의 처리 : 테스트 데이터에서 새로운 단어나 정보가 반복적으로 등장할 수 있는데, 이때 학습을 통해 파라미터를 업뎃하지 않으면 해당 단어를 예측할 확률이 매우 낮게 나올 것 .. 그러면 모델 성능을 과소평가하게 될 것이다.</li> <li>long-term memory : 문맥에 대한 정보를 network의 가중치에 저장하도록 유도해서, 검증 단계에서도 모델이 점진적으로 학습하도록 만든다.</li> </ol> <p>즉, dynamic model은 새로운 도메인에 자동적으로 적응할 수 있을 것이다.</p> <p>dynamic model에서는 learning rate =0.1로 픽스된 값을 사용한다. 훈련 시 모든 데이터는 각각 epoch에 제공되지만, dynamic model은 검증 단계에서 한 번만 업데이트된다 (optimal한 솔루션은 당연히 아니겠지만, 이것만으로도 static model에 비해 perplexity reduction을 크게 얻을 수 있다)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>perplexity reduction이란? 

모델이 uncertainty를 얼마나 잘 줄이는지를 평가하는 것이다. 즉, perplexity 값이 작을수록 좋은 것. Reduction이 목표가 되는 것. 
</code></pre></div></div> <h3 id="최적화"> <a href="#최적화" class="anchor-heading" aria-labelledby="최적화"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 최적화 </h3> <p>성능 향상을 위해, 훈련 text들에서 특정한 임계값을 설정하고 그보다 더 적은 횟수로 발생하는 단어들을 rare token으로 합친다. 그 단어의 확률은 다음과 같이 계산된다. (짜잘한 것들은 한번에 묶어서 따로 처리하는 느낌).</p> <p>[ P(w_{i}(t+1)|w(t), s(t-1)) = \begin{cases} y_{\text{rare}}(t) C_{\text{rare}} &amp; \text{if } w_i(t + 1) \text{ is rare} <br /> y_i(t) &amp; \text{otherwise} \end{cases} \tag{7} ]</p> <p>특정 임계값보다 적은 확률로 발생하는 애들은 그냥 똑같이 취급된다는 것이 특징이다.</p><hr /> <h3> <a href="#최적화" class="anchor-heading" aria-labelledby="최적화"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🔗 Related Posts in Paper </h3> <ul> <li><a href="/paper/2025/01/07/Attention.html">[Attention] Neural Machine Translation by Jointly Learning to Align and Translate (2015)</a> (2025-01-07)</li> <li><a href="/paper/2025/01/06/Seq2Seq.html">[Seq2Seq] Sequence to Sequence Learning with Neural Networks (2014)</a> (2025-01-06)</li> <li><a href="/paper/2025/01/05/LSTM.html">[LSTM] Long Short Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling (2014)</a> (2025-01-05)</li> <li><a href="/paper/2025/01/04/Word2Vec.html">[Word2Vec] Efficient Estimation of Word Representations in Vector Space (2013)</a> (2025-01-04)</li> </ul> <nav class="post-nav"> <a class="prev" href="/eda/2025/01/02/%EC%84%9C%EC%9A%B8%EC%8B%9C-%EB%B2%84%EC%8A%A4-%EB%85%B8%EC%84%A0-%EC%A0%95%EB%A5%98%EC%9E%A5-%EC%8A%B9%ED%95%98%EC%B0%A8-%EB%B6%84%EC%84%9D-01.md.html">← 서울시 버스 노선 정류장 승하차 분석 - 01</a> <a class="next" href="/paper/2025/01/04/Word2Vec.html">[Word2Vec] Efficient Estimation of Word Representations in Vector Space (2013) →</a> </nav> </main> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
