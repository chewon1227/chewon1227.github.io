<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav ul li a { background-image: none; } .site-nav > ul.nav-category-list > li > button svg { transform: rotate(-90deg); } .site-nav > ul.nav-category-list > li.nav-list-item > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>[Attention] Neural Machine Translation by Jointly Learning to Align and Translate (2015) | Rachel Docs</title> <meta name="generator" content="Jekyll v4.3.4" /> <meta property="og:title" content="[Attention] Neural Machine Translation by Jointly Learning to Align and Translate (2015)" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="1. Intro" /> <meta property="og:description" content="1. Intro" /> <link rel="canonical" href="http://localhost:4000/paper/2025/03/29/Attention.html" /> <meta property="og:url" content="http://localhost:4000/paper/2025/03/29/Attention.html" /> <meta property="og:site_name" content="Rachel Docs" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2025-03-29T00:00:00+09:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="[Attention] Neural Machine Translation by Jointly Learning to Align and Translate (2015)" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-03-29T00:00:00+09:00","datePublished":"2025-03-29T00:00:00+09:00","description":"1. Intro","headline":"[Attention] Neural Machine Translation by Jointly Learning to Align and Translate (2015)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/paper/2025/03/29/Attention.html"},"url":"http://localhost:4000/paper/2025/03/29/Attention.html"}</script> <!-- End Jekyll SEO tag --> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <!-- KaTeX for math rendering --> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, { delimiters: [ {left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false} ] });"> </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Rachel Docs </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"> <li class="nav-list-item"> <a href="/" class="nav-list-link">Home</a> </li> <li class="nav-list-item"> <a href="/categories/dl.html" class="nav-list-link">DL</a> </li> <li class="nav-list-item"> <a href="/categories/nlp.html" class="nav-list-link">NLP</a> </li> <li class="nav-list-item"> <a href="/categories/eda.html" class="nav-list-link">EDA</a> </li> <li class="nav-list-item"> <a href="/categories/paper.html" class="nav-list-link">PAPER</a> </li> </ul> </nav> <nav aria-label="Categories" id="category-nav" class="site-nav"> <ul class="nav-list nav-category-list"> <li class="nav-list-item"> <button class="nav-list-expander btn-reset" aria-label="Toggle category DL" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <div class="nav-category">DL</div> <ul class="nav-list"> <li class="nav-list-item"> <a href="/dl/tensorflow-04/" class="nav-list-link"> TensorFlow를 이용한 Deep Learning 04 </a> </li> <li class="nav-list-item"> <a href="/dl/tensorflow-03/" class="nav-list-link"> TensorFlow를 이용한 Deep Learning 03 </a> </li> <li class="nav-list-item"> <a href="/dl/tensorflow-02/" class="nav-list-link"> TensorFlow를 이용한 Deep Learning 02 </a> </li> <li class="nav-list-item"> <a href="/dl/tensorflow-01/" class="nav-list-link"> TensorFlow를 이용한 Deep Learning 01 </a> </li> </ul> </li> </ul> <ul class="nav-list nav-category-list"> <li class="nav-list-item"> <button class="nav-list-expander btn-reset" aria-label="Toggle category EDA" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <div class="nav-category">EDA</div> <ul class="nav-list"> <li class="nav-list-item"> <a href="/eda/bus-eda-01/" class="nav-list-link"> 서울시 버스 노선 정류장 승하차 분석 - 01 </a> </li> </ul> </li> </ul> <ul class="nav-list nav-category-list"> <li class="nav-list-item"> <button class="nav-list-expander btn-reset" aria-label="Toggle category NLP" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <div class="nav-category">NLP</div> <ul class="nav-list"> <li class="nav-list-item"> <a href="/nlp/react/" class="nav-list-link"> ReAct: Synergizing Reasoning and Acting in Language Models </a> </li> <li class="nav-list-item"> <a href="/nlp/gpt1/" class="nav-list-link"> [GPT-1] Improving Language Understanding by Generative Pre-Training(2018) </a> </li> <li class="nav-list-item"> <a href="/nlp/batchapi/" class="nav-list-link"> 일반 API와 Batch API 사용하기 </a> </li> <li class="nav-list-item"> <a href="/nlp/colab-finetune/" class="nav-list-link"> Colab으로 파인튜닝(Fine-Tuning)하기 </a> </li> <li class="nav-list-item"> <a href="/nlp/colab-prompting/" class="nav-list-link"> Colab으로 프롬프팅(Prompting)하기 </a> </li> </ul> </li> </ul> <ul class="nav-list nav-category-list"> <li class="nav-list-item"> <button class="nav-list-expander btn-reset" aria-label="Toggle category Paper" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button> <div class="nav-category">Paper</div> <ul class="nav-list"> <li class="nav-list-item"> <a href="/paper/2025/03/29/Attention.html" class="nav-list-link"> [Attention] Neural Machine Translation by Jointly Learning to Align and Translate (2015) </a> </li> <li class="nav-list-item"> <a href="/nlp/seq2seq/" class="nav-list-link"> Sequence-to-Sequence Models </a> </li> <li class="nav-list-item"> <a href="/paper/2025/02/24/LSTM.html" class="nav-list-link"> [LSTM] Long Short Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling (2014) </a> </li> <li class="nav-list-item"> <a href="/paper/2025/02/14/Word2Vec.html" class="nav-list-link"> [Word2Vec] Efficient Estimation of Word Representations in Vector Space (2013) </a> </li> <li class="nav-list-item"> <a href="/paper/2025/02/05/RNN.html" class="nav-list-link"> [RNN] Recurrent neural network based language model (2010) </a> </li> </ul> </li> </ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Rachel Docs" aria-label="Search Rachel Docs" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/just-the-docs/just-the-docs-template" class="site-button" > Template Repository </a> </li> </ul> </nav> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1>[Attention] Neural Machine Translation by Jointly Learning to Align and Translate (2015)</h1> <h2 id="1-intro"> <a href="#1-intro" class="anchor-heading" aria-labelledby="1-intro"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. Intro </h2> <p><strong>Neural Machine Translation</strong>은 기계번역에서 새롭게 등장하는 접근이다.</p> <p>기존에 있었던 Traditional Phrase-based translation system의 경우 sub component들이 있고 각각 tune되어야 했지만, 이 neural machine translation은 1개의 거대한 신경망 구조이다 (확실히 학습 시 효율적일 것)</p> <p>일반적으로 기계번역 모델이라고 하면 encoder-decoder 형식이고, 각 언어가 encoder, decoder을 각각 차지한다. encoder이 기존 문장을 fixed-length 벡터로 읽어들인 후, decoder이 그에 대한 번역을 도출한다. 여기서 fixed-length 벡터로 읽어들인다는 점이, 긴 문장을 대하기 어렵다는 점에서 한계점이다. 특히 훈련 코퍼스에서 긴 문장이 있을 시 제대로 학습이 안될 것이다.</p> <p>그래서 align과 translate를 동시에 학습하는 encoder-decoder model을 제시한다. • translation으로 단어를 생성해서 제시함 • 가장 관련된 정보가 집중되어 있는 source sentence에서의 set of positions 를 탐색함 • source position과 이전에 생성된 모든 target words 를 기반으로 target word를 예측함</p> <p>특히 가장 주목할만한 특징은 fixed-length 구조가 아니라는 것이다. 즉, 그 길이에 맞춰서 자를 필요가 없어졌으니 긴 문장에 더욱 강력한 성능을 뽐낼 것이다.</p><hr /> <h2 id="2-background"> <a href="#2-background" class="anchor-heading" aria-labelledby="2-background"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. Background </h2> <p>확률적 관점에서 보면 기계번역은, source sentence $x$에 대해서 target sentence의 conditional probability $y$를 최대화하는 것이다. 이를 위해 신경망 구조를 활용하는 것이 대두되었고, 특히 RNN을 두 개 사용하는 방식으로 구현된다. • 한 RNN은 다양한 길이의 source sentence를 fixed-length 벡터로 encode하는 데에 사용 • 나머지 RNN은 다시 다양한 길이의 target sentence로 decode하는 데에 사용</p> <p>즉, 정리하면 variable-length → fixed length → variable-length 로 가는 것이다.</p> <h3 id="2-1-rnn-encoder-decoder"> <a href="#2-1-rnn-encoder-decoder" class="anchor-heading" aria-labelledby="2-1-rnn-encoder-decoder"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2-1. RNN Encoder-Decoder </h3> <p>encoder이 input sentence를 읽는다 - $x = (x_1, \ldots, x_{T_x})$ 를 $c^2$로 변환해서! hidden layer은 일반적으로 $h_t = f(x_t, h_{t-1})$ 와 같은 형태로 구성된다. 그리고</p> \[c = q({h_1, \ldots, h_{T_x}})\] <p>를 통해 context vector $c$를 계산한다.</p> <p>decoder은 $c$와 이전에 만들어진 단어들 ${y_1, \ldots, y_{t’-1}}$ 을 고려하여 다음 단어인 $y$를 예측한다.</p> \[p(y) = \prod_{t=1}^{T} p(y_t \mid {y_1, \ldots, y_{t-1}}, c),\] \[p(y_t \mid {y_1, \ldots, y_{t-1}}, c) = g(y_{t-1}, s_t, c),\] <p>여기서 함수 $g$는 확률을 끌어내기 위한 비선형 함수가 될 것이다.</p><hr /> <h2 id="3-learning-to-align-and-translate"> <a href="#3-learning-to-align-and-translate" class="anchor-heading" aria-labelledby="3-learning-to-align-and-translate"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. Learning to Align and Translate </h2> <p>align과 translate를 어떻게 동시에 학습할 수 있을까? 이를 위해 새로운 구조를 도입한다. • encoder : bidirectional RNN • decoder : source sentence를 탐색하는 구조를 모방</p> <h3 id="3-1-decoder"> <a href="#3-1-decoder" class="anchor-heading" aria-labelledby="3-1-decoder"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3-1. Decoder </h3> <p>기존 RNN Encoder-Decoder 모델에서는</p> \[p(y) = \prod_{t=1}^{T} p(y_t \mid {y_1, \ldots, y_{t-1}}, c),\] <p>로 정의했던 $p(y)$를 이제는</p> \[p(y_i \mid y_1, \ldots, y_{i-1}, x) = g(y_{i-1}, s_i, c_i),\] <p>로 정의한다. 가장 큰 차이점은, 각 단어 $y_i$마다 독립적인 context vector $c_i$를 사용한다는 것이다.</p> <p>각각의 $c_i$는 encoder이 생성한 annotation의 순서에 따라 계산된다. 이때 annotation은 전체 input 순서에 대한 정보를 가지고 있으며 특히, $i$번째 단어 주변에 강하게 초점이 맞추어져 있다. $c_i$는 다음과 같이 계산된다.</p> \[c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j\] <p>여기서 가중합 $\alpha_{ij}$는 다음과 같이 계산된다.</p> \[\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}\] <p>$\alpha$는 translation 단어를 생성할 때 얼마나 각 context에 attention할 건지를 나타낸다. 이는 alignment model이라고 할 수 있는데 input의 $j$ 번째 단어와 output의 $i$ 번째 단어가 얼마나 매칭되는지를 점수매긴다.</p> <p>즉, decoder는 source sentence에서 어느 위치의 단어에 더 attention을 줄지를 결정할 수 있다.</p> <h3 id="3-2-encoder--bidirectional-rnn-for-annotating-sequence"> <a href="#3-2-encoder--bidirectional-rnn-for-annotating-sequence" class="anchor-heading" aria-labelledby="3-2-encoder--bidirectional-rnn-for-annotating-sequence"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3-2. Encoder : Bidirectional RNN for Annotating Sequence </h3> <p>encoder에서는 기존과는 달리 양방향으로 읽어나간다. • Forward RNN의 경우 순서대로 읽어나가며 forward hidden state를 생성한다 • Backward RNN의 경우 거꾸로 읽어나가며 backward hidden state를 생성한다</p><hr /> <h2 id="4-qualitative-analysis"> <a href="#4-qualitative-analysis" class="anchor-heading" aria-labelledby="4-qualitative-analysis"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 4. Qualitative Analysis </h2> <h3 id="4-1-alignment"> <a href="#4-1-alignment" class="anchor-heading" aria-labelledby="4-1-alignment"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 4-1. Alignment </h3> <p>weight $\alpha_{ij}$ 를 통해 soft-alignment를 찾을 수 있게 되었다.</p> <h3 id="4-2-long-sentences"> <a href="#4-2-long-sentences" class="anchor-heading" aria-labelledby="4-2-long-sentences"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 4-2. Long Sentences </h3> <p>기존 RNN은 문장이 길 경우 후반부 번역이 흐려졌지만, 새 모델은 잘 해낸다.</p><hr /> <h2 id="5-model-architecture"> <a href="#5-model-architecture" class="anchor-heading" aria-labelledby="5-model-architecture"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 5. Model Architecture </h2> <h3 id="5-1-recurrent-neural-network---gated-hidden-unit"> <a href="#5-1-recurrent-neural-network---gated-hidden-unit" class="anchor-heading" aria-labelledby="5-1-recurrent-neural-network---gated-hidden-unit"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 5-1. Recurrent Neural Network - Gated Hidden Unit </h3> <p>activation function $f$ 자리에는 gated hidden unit이 들어간다. LSTM과 유사한 구조로 long-term dependency를 잘 학습한다.</p> \[s_i = (1 - z_i) \odot s_{i-1} + z_i \odot \tilde{s_i},\] \[z_i = \sigma(W_z e(y_{i-1}) + U_z s_{i-1} + C_z c_i), \quad r_i = \sigma(W_r e(y_{i-1}) + U_r s_{i-1} + C_r c_i)\] <h3 id="5-2-alignment-model"> <a href="#5-2-alignment-model" class="anchor-heading" aria-labelledby="5-2-alignment-model"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 5-2. Alignment Model </h3> \[a(s_{i-1}, h_j) = v^T \tanh(W_a s_{i-1} + U_a h_j)\] <h3 id="5-3-encoder"> <a href="#5-3-encoder" class="anchor-heading" aria-labelledby="5-3-encoder"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 5-3. Encoder </h3> <p>입력 $x = (x_1, \ldots, x_{T_x}), \quad x_i \in \mathbb{R}^{K_x}$ 출력 $y = (y_1, \ldots, y_{T_y}), \quad y_i \in \mathbb{R}^{K_y}$</p> <p>Forward state (bidirectional RNN)</p> \[\overrightarrow{h_i} = (1 - \overrightarrow{z_i}) \odot \overrightarrow{h_{i-1}} + \overrightarrow{z_i} \odot \tilde{\overrightarrow{h_i}},\] \[\tilde{\overrightarrow{h_i}} = \tanh(\overrightarrow{W} E x_i + \overrightarrow{U} [\overrightarrow{r_i} \odot \overrightarrow{h_{i-1}}]),\] \[\overrightarrow{z_i} = \sigma(\overrightarrow{W_z} E x_i + \overrightarrow{U_z} \overrightarrow{h_{i-1}}),\] \[\overrightarrow{r_i} = \sigma(\overrightarrow{W_r} E x_i + \overrightarrow{U_r} \overrightarrow{h_{i-1}})\] <h3 id="5-4-decoder"> <a href="#5-4-decoder" class="anchor-heading" aria-labelledby="5-4-decoder"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 5-4. Decoder </h3> \[\tilde{s_i} = \tanh(W E y_{i-1} + U [r_i \odot s_{i-1}] + C c_i),\] \[z_i = \sigma(W_z E y_{i-1} + U_z s_{i-1} + C_z c_i), \quad r_i = \sigma(W_r E y_{i-1} + U_r s_{i-1} + C_r c_i)\] \[c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j\] <h2 id="6-정리"> <a href="#6-정리" class="anchor-heading" aria-labelledby="6-정리"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 6. 정리 </h2> <div class="callout"> • Attention이란 → hidden layer 정보를 기반으로, 다음 단어 예측 시 중요한 정보를 집중하는 메커니즘 • 수행 과정 → alignment 계산 → attention weight 계산 → context vector 생성 → decoder 업데이트 </div><hr /> <h3> <a href="#6-정리" class="anchor-heading" aria-labelledby="6-정리"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🔗 Related Posts in Paper </h3> <ul> <li><a href="/nlp/seq2seq/">Sequence-to-Sequence Models</a> (2025-03-15)</li> <li><a href="/paper/2025/02/24/LSTM.html">[LSTM] Long Short Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling (2014)</a> (2025-02-24)</li> <li><a href="/paper/2025/02/14/Word2Vec.html">[Word2Vec] Efficient Estimation of Word Representations in Vector Space (2013)</a> (2025-02-14)</li> <li><a href="/paper/2025/02/05/RNN.html">[RNN] Recurrent neural network based language model (2010)</a> (2025-02-05)</li> </ul> <nav class="post-nav"> <a class="prev" href="/nlp/seq2seq/">← Sequence-to-Sequence Models</a> <a class="next" href="/nlp/batchapi/">일반 API와 Batch API 사용하기 →</a> </nav> </main> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
